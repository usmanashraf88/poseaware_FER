{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e7004b-03f1-41fd-8ad0-d06cfac0106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import timm  # For Vision Transformer (ViT)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Define dataset class\n",
    "class RAFDBDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = glob(os.path.join(root_dir, \"*.jpg\"))\n",
    "        self.labels = [int(os.path.basename(p).split(\"_\")[0]) for p in self.image_paths]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "# Data transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# Load Dataset\n",
    "dataset = RAFDBDataset(root_dir=\"path_to_rafdb\", transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Feature Extractor (CNN + ViT)\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.efficientnet = models.efficientnet_b0(pretrained=True)\n",
    "        self.efficientnet = nn.Sequential(*list(self.efficientnet.children())[:-1])\n",
    "\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n",
    "\n",
    "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "        self.vit.head = nn.Identity()  # Remove classification head\n",
    "\n",
    "        self.fc = nn.Linear(1280 + 2048 + 768, 512)  # Concatenate features\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat1 = self.efficientnet(x).flatten(start_dim=1)\n",
    "        feat2 = self.resnet(x).flatten(start_dim=1)\n",
    "        feat3 = self.vit(x)\n",
    "        combined_feat = torch.cat([feat1, feat2, feat3], dim=1)\n",
    "        return self.fc(combined_feat)\n",
    "\n",
    "# Pose Normalization (STN)\n",
    "class STN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(STN, self).__init__()\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=7),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(32 * 53 * 53, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 6)\n",
    "        )\n",
    "        self.fc_loc[2].weight.data.fill_(0)\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 32 * 53 * 53)\n",
    "        theta = self.fc_loc(xs).view(-1, 2, 3)\n",
    "        grid = torch.nn.functional.affine_grid(theta, x.size())\n",
    "        return torch.nn.functional.grid_sample(x, grid)\n",
    "\n",
    "# Define Model with STN and Feature Extraction\n",
    "class PoseAwareFER(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoseAwareFER, self).__init__()\n",
    "        self.stn = STN()\n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "        self.classifier = nn.Linear(512, 7)  # Assuming 7 classes in RAF-DB\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stn(x)\n",
    "        features = self.feature_extractor(x)\n",
    "        return self.classifier(features)\n",
    "\n",
    "# Train Function\n",
    "def train_model(model, train_loader, test_loader, num_epochs=10, learning_rate=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = PoseAwareFER()\n",
    "trained_model = train_model(model, train_loader, test_loader)\n",
    "\n",
    "# Extract deep features and train XGBoost\n",
    "def extract_features(model, dataloader):\n",
    "    model.eval()\n",
    "    features, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, lbls in dataloader:\n",
    "            outputs = model.feature_extractor(images)\n",
    "            features.append(outputs.cpu().numpy())\n",
    "            labels.append(lbls.cpu().numpy())\n",
    "    return np.vstack(features), np.hstack(labels)\n",
    "\n",
    "train_features, train_labels = extract_features(trained_model, train_loader)\n",
    "test_features, test_labels = extract_features(trained_model, test_loader)\n",
    "\n",
    "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1)\n",
    "xgb_model.fit(train_features, train_labels)\n",
    "xgb_preds = xgb_model.predict(test_features)\n",
    "\n",
    "final_accuracy = accuracy_score(test_labels, xgb_preds)\n",
    "print(f\"Final XGBoost Accuracy: {final_accuracy:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
